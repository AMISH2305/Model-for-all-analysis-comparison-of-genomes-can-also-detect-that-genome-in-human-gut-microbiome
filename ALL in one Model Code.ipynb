{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396311d-2ebb-47d0-9f28-66127ab061c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the file of genome which you want to search in the human gut microbiome: entamoeba_histolytica_filtered.fasta\n",
      "Enter the file of genome which you want to compare with the first one: moshkovskii_filtered.fna\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics:\n",
      "Genome 1:\n",
      "count    2368.000000\n",
      "mean       14.595712\n",
      "std         8.508614\n",
      "min         0.000000\n",
      "25%         8.858446\n",
      "50%        13.764197\n",
      "75%        18.478574\n",
      "max        50.758853\n",
      "dtype: float64\n",
      "Number of ORFs: 8994\n",
      "\n",
      "Genome 2:\n",
      "count    4607.000000\n",
      "mean       20.281315\n",
      "std         7.748576\n",
      "min         0.144718\n",
      "25%        15.428746\n",
      "50%        21.344233\n",
      "75%        25.576080\n",
      "max        52.338530\n",
      "dtype: float64\n",
      "Number of ORFs: 16394\n",
      "\n",
      "Statistical Tests:\n",
      "t-test for GC content between the two genomes:\n",
      "t-statistic: -28.055592198053688\n",
      "p-value: 3.4171753906103687e-164\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aamis\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 253ms/step - accuracy: 0.9735 - loss: 0.0192 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 6.9388e-30 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 241ms/step - accuracy: 1.0000 - loss: 9.1927e-30 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 242ms/step - accuracy: 1.0000 - loss: 4.5429e-30 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 247ms/step - accuracy: 1.0000 - loss: 1.0820e-29 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 7.6886e-30 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 247ms/step - accuracy: 1.0000 - loss: 5.4247e-30 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 248ms/step - accuracy: 1.0000 - loss: 1.0240e-29 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 4.6162e-31 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 249ms/step - accuracy: 1.0000 - loss: 1.2892e-29 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Function to calculate GC content\n",
    "def calculate_gc_content(sequence):\n",
    "    \"\"\"Calculates the GC content (percentage of G and C nucleotides) of a DNA sequence.\"\"\"\n",
    "    gc_count = sequence.count('G') + sequence.count('C')\n",
    "    return (gc_count / len(sequence)) * 100\n",
    "\n",
    "# Function to find open reading frames (ORFs) in a DNA sequence\n",
    "def find_orfs(sequence, min_length=100):\n",
    "    orfs = []\n",
    "    start_codon = \"ATG\"\n",
    "    stop_codons = [\"TAA\", \"TAG\", \"TGA\"]\n",
    "    in_orf = False\n",
    "    current_orf = \"\"\n",
    "\n",
    "    for i in range(0, len(sequence), 3):\n",
    "        codon = sequence[i:i+3]\n",
    "        if codon == start_codon:\n",
    "            if not in_orf:\n",
    "                in_orf = True\n",
    "                current_orf = start_codon\n",
    "            else:\n",
    "                current_orf += start_codon\n",
    "        elif codon in stop_codons:\n",
    "            if in_orf:\n",
    "                in_orf = False\n",
    "                if len(current_orf) >= min_length:\n",
    "                    orfs.append((current_orf, i - len(current_orf) * 3, i))\n",
    "                current_orf = \"\"\n",
    "        elif in_orf:\n",
    "            current_orf += codon\n",
    "\n",
    "    return orfs\n",
    "\n",
    "# Function to encode DNA sequences into numerical format\n",
    "def encode_sequence(seq):\n",
    "    mapping = {'A': 1, 'C': 2, 'G': 3, 'T': 4,'a':1, 'c':2, 'g':3, 't':4}\n",
    "    return [mapping[base] for base in seq]\n",
    "\n",
    "# Function to extract ORFs and encode them\n",
    "def extract_and_encode_orfs(file_path, min_length=100):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        orfs = find_orfs(str(record.seq), min_length)\n",
    "        for orf, start, end in orfs:\n",
    "            encoded_orf = encode_sequence(orf)\n",
    "            sequences.append(encoded_orf)\n",
    "    return sequences\n",
    "\n",
    "# Function to pad sequences to the same length\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Function to train the deep learning model\n",
    "def train_model(orfs):\n",
    "    # Define the maximum length of ORFs\n",
    "    maxlen = max(len(orf) for orf in orfs)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    X = pad_sequences(orfs, maxlen=maxlen)\n",
    "    \n",
    "    # Create labels (all ones since these are true ORFs)\n",
    "    y = np.ones((len(orfs), 1))\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5, output_dim=8, input_length=maxlen),\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "    \n",
    "    return model, maxlen\n",
    "\n",
    "# Function to predict ORFs in a new genome\n",
    "def predict_orfs(model, file_path, maxlen):\n",
    "    orfs = extract_and_encode_orfs(file_path)\n",
    "    X = pad_sequences(orfs, maxlen=maxlen)\n",
    "    predictions = model.predict(X)\n",
    "    predicted_orfs = [orfs[i] for i in range(len(predictions)) if predictions[i] > 0.5]\n",
    "    return predicted_orfs\n",
    "def analyze_genome(file_path):\n",
    "    gc_contents = []\n",
    "    lengths = []\n",
    "    orf_stats = {\"total_length\": 0, \"total_count\": 0}\n",
    "\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        gc_content = calculate_gc_content(record.seq)\n",
    "        gc_contents.append(gc_content)\n",
    "        lengths.append(len(record.seq))\n",
    "        orfs1 = find_orfs(str(record.seq))\n",
    "        \n",
    "        for orf, start, end in orfs1:\n",
    "            orf_length = len(orf)\n",
    "            orf_stats[\"total_length\"] += orf_length\n",
    "            orf_stats[\"total_count\"] += 1\n",
    "\n",
    "    avg_gc_content = sum(gc_contents) / len(gc_contents)\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    avg_orf_length = orf_stats[\"total_length\"] / orf_stats[\"total_count\"] if orf_stats[\"total_count\"] > 0 else 0\n",
    "    return avg_gc_content, avg_length, avg_orf_length, gc_contents, orf_stats[\"total_count\"]\n",
    "\n",
    "def compare_genomes(file_path1, file_path2):\n",
    "    # Analyze the first genome and capture the list of GC contents and number of ORFs\n",
    "    _, _, _, gc_contents1, orf_count1 = analyze_genome(file_path1)\n",
    "    # Analyze the second genome and capture the list of GC contents and number of ORFs\n",
    "    _, _, _, gc_contents2, orf_count2 = analyze_genome(file_path2)\n",
    "\n",
    "    # Descriptive statistics\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(\"Genome 1:\")\n",
    "    print(pd.Series(gc_contents1).describe())\n",
    "    print(\"Number of ORFs:\", orf_count1)\n",
    "    print(\"\\nGenome 2:\")\n",
    "    print(pd.Series(gc_contents2).describe())\n",
    "    print(\"Number of ORFs:\", orf_count2)\n",
    "\n",
    "    # Statistical tests\n",
    "    print(\"\\nStatistical Tests:\")\n",
    "    # t-test for GC content between the two genomes\n",
    "    print(\"t-test for GC content between the two genomes:\")\n",
    "    t_statistic, p_value = stats.ttest_ind(gc_contents1, gc_contents2)\n",
    "    print(\"t-statistic:\", t_statistic)\n",
    "    print(\"p-value:\", p_value)\n",
    "\n",
    "def remove_ns_from_fasta(input_fasta, output_fasta1):\n",
    "      output_fasta1 = \"Filtered_Human_Gut_Microbiome\"\n",
    "      with open(output_fasta, \"w\") as output_handle:\n",
    "                for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
    "            # Remove 'N's from the sequence\n",
    "                    cleaned_seq = str(record.seq).replace(\"N\", \"\")\n",
    "            # Update the sequence in the record\n",
    "                    record.seq = cleaned_seq\n",
    "            # Write the cleaned record to the output file\n",
    "                    SeqIO.write(record, output_handle, \"fasta\")\n",
    "\n",
    "      print(f\"Cleaned sequences saved to {output_fasta}\")\n",
    "      return output_fast1\n",
    "def main():\n",
    "    # Provide the path to the first genome FASTA file here\n",
    "    file_path1 = input(\"Enter the file of genome which you want to search in the human gut microbiome:\")\n",
    "    file_path2 = input(\"Enter the file of genome which you want to compare with the first one:\")\n",
    "    # Extract and encode ORFs from the first genome\n",
    "    analyze_genome(file_path1)\n",
    "    analyze_genome(file_path2)\n",
    "    compare_genomes(file_path1, file_path2)\n",
    "    orfs = extract_and_encode_orfs(file_path1)\n",
    "\n",
    "    # Train the deep learning model\n",
    "    model, maxlen = train_model(orfs)\n",
    "\n",
    "    # Provide the path to the human gut microbiome FASTA file here\n",
    "    file_path_human_gut = input(\"Enter the path to the human gut microbiome FASTA file: \")\n",
    "    output_fasta = remove_ns_from_fasta(file_path_human_gut)\n",
    "    # Predict ORFs in the human gut microbiome genome\n",
    "    predicted_orfs = predict_orfs(model, output_fasta, maxlen)\n",
    "\n",
    "    print(\"Predicted ORFs in the human gut microbiome genome:\")\n",
    "    for orf in predicted_orfs:\n",
    "        print(\"Found\")\n",
    "        print(orf)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ed481-d205-4637-a88d-144c07100247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
